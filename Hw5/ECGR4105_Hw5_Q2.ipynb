{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN2QJetSdMMmYN2p6f2KROG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/C-Little26/ECGR4105/blob/main/Hw5/ECGR4105_Hw5_Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SXiZzHH_4ia9",
        "outputId": "8556eadb-6370-48ff-b2af-0fac3ef57368"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "#necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sea\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import tensorflow\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import datasets\n",
        "from sklearn import metrics\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = '/content/drive/My Drive/Machine Learning/Datasets/Housing.csv'\n",
        "housing = pd.DataFrame(pd.read_csv(filepath))\n",
        "housing.isnull().sum()*100/housing.shape[0] #use to check for null values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "elTY_eCM4nWr",
        "outputId": "3a4ee88f-0436-4dc6-e71b-4d5b0539a6dd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "price               0.0\n",
              "area                0.0\n",
              "bedrooms            0.0\n",
              "bathrooms           0.0\n",
              "stories             0.0\n",
              "mainroad            0.0\n",
              "guestroom           0.0\n",
              "basement            0.0\n",
              "hotwaterheating     0.0\n",
              "airconditioning     0.0\n",
              "parking             0.0\n",
              "prefarea            0.0\n",
              "furnishingstatus    0.0\n",
              "dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#variables used for this model\n",
        "varlist =  ['area', 'bedrooms', 'bathrooms', 'stories', 'parking']\n",
        "#establish matrices\n",
        "Y = housing.pop('price')\n",
        "X = housing[varlist]\n",
        "#train/test split\n",
        "x_train, x_val, y_train, y_val = train_test_split(X,Y, train_size = 0.8, test_size = 0.2, random_state=0)"
      ],
      "metadata": {
        "id": "2kIvHWYT46O8"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#standardize values\n",
        "sc_x = StandardScaler()\n",
        "x_train = sc_x.fit_transform(x_train)\n",
        "x_val = sc_x.fit_transform(x_val)"
      ],
      "metadata": {
        "id": "qN94B9i36O1p"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stack arrays before conversion\n",
        "x_t_arr = np.c_[np.ones((len(y_train), 1)), x_train]\n",
        "x_v_arr = np.c_[np.ones((len(y_val),1)), x_val]\n",
        "\n",
        "#convert to tensors\n",
        "x_t_ten = torch.tensor(x_t_arr)\n",
        "x_v_ten = torch.tensor(x_v_arr)\n",
        "y_t_ten = torch.tensor(y_train.values)\n",
        "y_v_ten = torch.tensor(y_val.values)\n",
        "#have to call .values for it to be treated like an array"
      ],
      "metadata": {
        "id": "dHgTCFwH7L-u"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the model\n",
        "def model(t_u, w5, w4, w3, w2, w1, b):\n",
        "  return w5*t_u**5 + w4*t_u**4 + w3*t_u**3 + w2 * t_u ** 2 + w1 * t_u + b\n",
        "\n",
        "#linear model to test something out later\n",
        "def linear(t_u, w, b):\n",
        "    return w * t_u + b\n",
        "\n",
        "#define the loss function\n",
        "def loss_fn(t_p, t_c):\n",
        "    squared_diffs = (t_p - t_c)**2\n",
        "    return squared_diffs.mean()\n",
        "\n",
        "#initial parameters\n",
        "params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)"
      ],
      "metadata": {
        "id": "s666P97N9aOz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#define the training loop\n",
        "def loop(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_val, t_c_val):\n",
        "  for epoch in range (1, n_epochs +1):\n",
        "\n",
        "    if params.grad is not None:  # <1>\n",
        "      params.grad.zero_()\n",
        "\n",
        "    t_p_train = model(t_u_train, *params)\n",
        "    loss_train = loss_fn(t_p_train.transpose(0,1), t_c_train)\n",
        "\n",
        "    t_p_val = model(t_u_val, *params)\n",
        "    loss_val = loss_fn(t_p_val.transpose(0,1), t_c_val)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss_train.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss: {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss: {loss_val.item():.4f}\\n\")\n",
        "  return params"
      ],
      "metadata": {
        "id": "5lzu8WNQ_RTG"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear training loop, yet again for testing something later\n",
        "def linear_loop(n_epochs, optimizer, params, t_u_train, t_c_train, t_u_val, t_c_val):\n",
        "    for epoch in range(1, n_epochs+1):\n",
        "        t_p_train = linear(t_u_train, *params)\n",
        "        loss_train = loss_fn(t_p_train.transpose(0,1), t_c_train)\n",
        "\n",
        "        t_p_val = linear(t_u_val, *params)\n",
        "        loss_val = loss_fn(t_p_val.transpose(0,1), t_c_val)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss_train.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if epoch % 500 == 0:\n",
        "            print(f\"Epoch {epoch}, Training loss: {loss_train.item():.4f},\"\n",
        "                  f\" Validation loss: {loss_val.item():.4f}\\n\")\n",
        "    return params"
      ],
      "metadata": {
        "id": "UuqEAMGnEKQU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#establish variables for model iterations\n",
        "learning_rate = [0.00001, 0.000001, 0.0000001, 0.00000001]\n",
        "sgd_params = torch.zeros(len(learning_rate), 6)\n",
        "sgd_loss = torch.zeros(len(learning_rate), 1)\n",
        "#run training for each learning rate using SGD\n",
        "for i in learning_rate:\n",
        "    params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "    optimizer = optim.SGD([params], lr=i)\n",
        "    print(f'\\nTraining with learning rate: {i}')\n",
        "    sgd_params[learning_rate.index(i)] = loop(\n",
        "        n_epochs = 5000, optimizer = optimizer,\n",
        "        params = params,\n",
        "        t_u_train = x_t_ten, t_u_val = x_v_ten,\n",
        "        t_c_train = y_t_ten, t_c_val = y_v_ten, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaYTDh7P_ZoM",
        "outputId": "f849ffb4-b702-4d4c-b0a2-8513d337c37c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate: 1e-05\n",
            "Epoch 500, Training loss: 24062939433895.2656, Validation loss: 22573483960989.0234\n",
            "\n",
            "Epoch 1000, Training loss: 22756123929035.6523, Validation loss: 21339278009590.9805\n",
            "\n",
            "Epoch 1500, Training loss: 21843127180528.6367, Validation loss: 20509121276043.1250\n",
            "\n",
            "Epoch 2000, Training loss: 21155111802891.4375, Validation loss: 19897552080648.2695\n",
            "\n",
            "Epoch 2500, Training loss: 20597387184874.6562, Validation loss: 19403807486485.5156\n",
            "\n",
            "Epoch 3000, Training loss: 20116806891972.0781, Validation loss: 18973722858142.0977\n",
            "\n",
            "Epoch 3500, Training loss: 19683494287205.6680, Validation loss: 18578605914666.5156\n",
            "\n",
            "Epoch 4000, Training loss: 19280615772127.4688, Validation loss: 18203574011512.0859\n",
            "\n",
            "Epoch 4500, Training loss: 18898644426215.6875, Validation loss: 17841143001456.6250\n",
            "\n",
            "Epoch 5000, Training loss: 18532149408804.8633, Validation loss: 17487723404393.4121\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-06\n",
            "Epoch 500, Training loss: 25816266826436.4805, Validation loss: 24300517191106.7969\n",
            "\n",
            "Epoch 1000, Training loss: 25583579044477.8672, Validation loss: 24067502831330.4727\n",
            "\n",
            "Epoch 1500, Training loss: 25361566066086.8125, Validation loss: 23846163661663.0781\n",
            "\n",
            "Epoch 2000, Training loss: 25149632902995.9102, Validation loss: 23635815709467.9805\n",
            "\n",
            "Epoch 2500, Training loss: 24947218105140.0781, Validation loss: 23435815969162.5312\n",
            "\n",
            "Epoch 3000, Training loss: 24753791938908.1484, Validation loss: 23245558738316.5000\n",
            "\n",
            "Epoch 3500, Training loss: 24568854178124.2539, Validation loss: 23064472847024.2930\n",
            "\n",
            "Epoch 4000, Training loss: 24391933114473.7969, Validation loss: 22892020640213.8398\n",
            "\n",
            "Epoch 4500, Training loss: 24222582984116.8008, Validation loss: 22727694961270.4141\n",
            "\n",
            "Epoch 5000, Training loss: 24060383924349.0898, Validation loss: 22571018603122.9180\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-07\n",
            "Epoch 500, Training loss: 26076414966550.4961, Validation loss: 24622265935758.8203\n",
            "\n",
            "Epoch 1000, Training loss: 26014267910840.6953, Validation loss: 24517324364009.3711\n",
            "\n",
            "Epoch 1500, Training loss: 25985816176485.2969, Validation loss: 24476436315070.3594\n",
            "\n",
            "Epoch 2000, Training loss: 25960867480424.9219, Validation loss: 24447560498288.8438\n",
            "\n",
            "Epoch 2500, Training loss: 25936377060317.9102, Validation loss: 24421738771705.8477\n",
            "\n",
            "Epoch 3000, Training loss: 25912037334828.8984, Validation loss: 24396888610797.2500\n",
            "\n",
            "Epoch 3500, Training loss: 25887816703088.1016, Validation loss: 24372424803655.4570\n",
            "\n",
            "Epoch 4000, Training loss: 25863711352587.2305, Validation loss: 24348169022842.1484\n",
            "\n",
            "Epoch 4500, Training loss: 25839720295977.0547, Validation loss: 24324063846890.8516\n",
            "\n",
            "Epoch 5000, Training loss: 25815843031023.4258, Validation loss: 24300092032562.2930\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-08\n",
            "Epoch 500, Training loss: 26383448958236.7461, Validation loss: 25071191739099.0586\n",
            "\n",
            "Epoch 1000, Training loss: 26314139547672.2344, Validation loss: 24975039884209.2500\n",
            "\n",
            "Epoch 1500, Training loss: 26258516916344.6836, Validation loss: 24896491823320.7109\n",
            "\n",
            "Epoch 2000, Training loss: 26213777797980.1211, Validation loss: 24832112695778.1211\n",
            "\n",
            "Epoch 2500, Training loss: 26177693020949.1211, Validation loss: 24779152575548.1328\n",
            "\n",
            "Epoch 3000, Training loss: 26148490094012.0195, Validation loss: 24735408431843.8164\n",
            "\n",
            "Epoch 3500, Training loss: 26124759721542.8164, Validation loss: 24699114002817.8086\n",
            "\n",
            "Epoch 4000, Training loss: 26105381076696.1523, Validation loss: 24668851395778.4883\n",
            "\n",
            "Epoch 4500, Training loss: 26089463092832.0273, Validation loss: 24643481734199.3203\n",
            "\n",
            "Epoch 5000, Training loss: 26076297211929.4297, Validation loss: 24622089030343.7266\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#establish variables for model iterations\n",
        "learning_rate = [0.00001, 0.000001, 0.0000001, 0.00000001]\n",
        "adam_params = torch.zeros(len(learning_rate), 6)\n",
        "adam_loss = torch.zeros(len(learning_rate), 1)\n",
        "#run training for each learning rate using SGD\n",
        "for i in learning_rate:\n",
        "    params = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 0.0], requires_grad = True)\n",
        "    optimizer = optim.Adam([params], lr=i)\n",
        "    print(f'\\nTraining with learning rate: {i}')\n",
        "    sgd_params[learning_rate.index(i)] = loop(\n",
        "        n_epochs = 5000, optimizer = optimizer,\n",
        "        params = params,\n",
        "        t_u_train = x_t_ten, t_u_val = x_v_ten,\n",
        "        t_c_train = y_t_ten, t_c_val = y_v_ten, )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP6EjcfCAYBp",
        "outputId": "47dc40f1-c56f-4c7d-8426-a42976e3104d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training with learning rate: 1e-05\n",
            "Epoch 500, Training loss: 26469775977544.0156, Validation loss: 25188977638937.3398\n",
            "\n",
            "Epoch 1000, Training loss: 26469774913841.3086, Validation loss: 25188976266172.1680\n",
            "\n",
            "Epoch 1500, Training loss: 26469773850140.7656, Validation loss: 25188974893409.2773\n",
            "\n",
            "Epoch 2000, Training loss: 26469772786437.2383, Validation loss: 25188973520643.6016\n",
            "\n",
            "Epoch 2500, Training loss: 26469771722734.0547, Validation loss: 25188972147878.4023\n",
            "\n",
            "Epoch 3000, Training loss: 26469770659031.7734, Validation loss: 25188970775114.2500\n",
            "\n",
            "Epoch 3500, Training loss: 26469769595337.0703, Validation loss: 25188969402357.7422\n",
            "\n",
            "Epoch 4000, Training loss: 26469768531645.4922, Validation loss: 25188968029604.4766\n",
            "\n",
            "Epoch 4500, Training loss: 26469767467954.8242, Validation loss: 25188966656852.2617\n",
            "\n",
            "Epoch 5000, Training loss: 26469766404265.0586, Validation loss: 25188965284101.0898\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-06\n",
            "Epoch 500, Training loss: 26469776937791.3594, Validation loss: 25188978878256.0820\n",
            "\n",
            "Epoch 1000, Training loss: 26469776836258.6250, Validation loss: 25188978747291.4023\n",
            "\n",
            "Epoch 1500, Training loss: 26469776734725.8945, Validation loss: 25188978616326.7344\n",
            "\n",
            "Epoch 2000, Training loss: 26469776633193.1797, Validation loss: 25188978485362.0781\n",
            "\n",
            "Epoch 2500, Training loss: 26469776531660.4688, Validation loss: 25188978354397.4297\n",
            "\n",
            "Epoch 3000, Training loss: 26469776430127.7656, Validation loss: 25188978223432.7891\n",
            "\n",
            "Epoch 3500, Training loss: 26469776328595.0703, Validation loss: 25188978092468.1602\n",
            "\n",
            "Epoch 4000, Training loss: 26469776227062.5898, Validation loss: 25188977961503.7422\n",
            "\n",
            "Epoch 4500, Training loss: 26469776125531.0195, Validation loss: 25188977830540.2305\n",
            "\n",
            "Epoch 5000, Training loss: 26469776023999.4688, Validation loss: 25188977699576.7305\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-07\n",
            "Epoch 500, Training loss: 26469777026574.0078, Validation loss: 25188978992738.8125\n",
            "\n",
            "Epoch 1000, Training loss: 26469777014001.8555, Validation loss: 25188978976486.3047\n",
            "\n",
            "Epoch 1500, Training loss: 26469777001429.7031, Validation loss: 25188978960233.7930\n",
            "\n",
            "Epoch 2000, Training loss: 26469776988857.5547, Validation loss: 25188978943981.2891\n",
            "\n",
            "Epoch 2500, Training loss: 26469776976285.4023, Validation loss: 25188978927728.7773\n",
            "\n",
            "Epoch 3000, Training loss: 26469776963713.2461, Validation loss: 25188978911476.2695\n",
            "\n",
            "Epoch 3500, Training loss: 26469776951141.1016, Validation loss: 25188978895223.7695\n",
            "\n",
            "Epoch 4000, Training loss: 26469776938568.9492, Validation loss: 25188978878971.2617\n",
            "\n",
            "Epoch 4500, Training loss: 26469776925996.7969, Validation loss: 25188978862718.7539\n",
            "\n",
            "Epoch 5000, Training loss: 26469776913424.6484, Validation loss: 25188978846466.2461\n",
            "\n",
            "\n",
            "Training with learning rate: 1e-08\n",
            "Epoch 500, Training loss: 26469777039073.3320, Validation loss: 25188979008911.6758\n",
            "\n",
            "Epoch 1000, Training loss: 26469777039025.5586, Validation loss: 25188979008864.4453\n",
            "\n",
            "Epoch 1500, Training loss: 26469777038977.7812, Validation loss: 25188979008817.2188\n",
            "\n",
            "Epoch 2000, Training loss: 26469777038930.0039, Validation loss: 25188979008769.9805\n",
            "\n",
            "Epoch 2500, Training loss: 26469777038882.2266, Validation loss: 25188979008722.7461\n",
            "\n",
            "Epoch 3000, Training loss: 26469777038834.4453, Validation loss: 25188979008675.5117\n",
            "\n",
            "Epoch 3500, Training loss: 26469777038786.6641, Validation loss: 25188979008628.2734\n",
            "\n",
            "Epoch 4000, Training loss: 26469777038738.8828, Validation loss: 25188979008581.0430\n",
            "\n",
            "Epoch 4500, Training loss: 26469777038691.1016, Validation loss: 25188979008533.8086\n",
            "\n",
            "Epoch 5000, Training loss: 26469777038643.3242, Validation loss: 25188979008486.5742\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train with a linear model?\n",
        "#learning_rate = [0.00001, 0.000001, 0.0000001, 0.00000001]\n",
        "#lin_params = torch.zeros(len(learning_rate), 2)\n",
        "#lin_loss = torch.zeros(len(learning_rate), 1)\n",
        "#run training for each learning rate using linear model\n",
        "#for i in learning_rate:\n",
        "#    params = torch.tensor([1.0, 0.0], requires_grad = True)\n",
        "#    optimizer = optim.SGD([params], lr=i)\n",
        "#    print(f'\\nTraining with learning rate: {i}')\n",
        "#    sgd_params[learning_rate.index(i)] = linear_loop(\n",
        "#       n_epochs = 5000, optimizer = optimizer,\n",
        "#       params = params,\n",
        " #       t_u_train = x_t_ten, t_u_val = x_v_ten,\n",
        " #       t_c_train = y_t_ten, t_c_val = y_v_ten, )\n",
        "\n",
        "#doesn't work"
      ],
      "metadata": {
        "id": "qa4hXKJECcmD"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LjNbNLhHEesK"
      },
      "execution_count": 11,
      "outputs": []
    }
  ]
}